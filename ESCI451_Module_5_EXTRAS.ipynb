{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10e8055",
   "metadata": {},
   "source": [
    "# Introduction to Python for Earth Scientists\n",
    "\n",
    "These notebooks have been developed by Calum Chamberlain, Finnigan Illsley-Kemp and John Townend at [Victoria University of Wellington-Te Herenga Waka](https://www.wgtn.ac.nz) for use by Earth Science graduate students. \n",
    "\n",
    "The notebooks cover material that we think will be of particular benefit to those students with little or no previous experience of computer-based data analysis. We presume very little background in command-line or code-based computing, and have compiled this material with an emphasis on general tasks that a grad student might encounter on a daily basis. \n",
    "\n",
    "In 2021, this material will be delivered at the start of Trimester 1 in conjunction with [ESCI451 Active Earth](https://www.wgtn.ac.nz/courses/esci/451/2021/offering?crn=32176). Space and pandemic alert levels permitting, interested students not enrolled in ESCI451 are encouraged to come along too but please contact Calum, Finn, or John first.\n",
    "\n",
    "| Notebook | Contents | Data |\n",
    "| --- | --- | --- |\n",
    "| [1A](ESCI451_Module_1A.ipynb) | Introduction to programming, Python, and Jupyter notebooks | - |\n",
    "| [1B](ESCI451_Module_1B.ipynb) | Basic data types and variables, getting data, and plotting with Matplotlib | Geodetic positions |\n",
    "| [2A](ESCI451_Module_2A.ipynb) | More complex plotting, introduction to Numpy | Geodetic positions; DFDP-2B temperatures |\n",
    "| [2B](ESCI451_Module_2B.ipynb) | Using Pandas to load, peruse and plot data | Earthquake catalogue  |\n",
    "| [3A](ESCI451_Module_3A.ipynb) | Working with Pandas dataframes | Geochemical data set; GNSS data |\n",
    "| [3B](ESCI451_Module_3B.ipynb) | Simple time series analysis using Pandas | Historical temperature records |\n",
    "| [4A](ESCI451_Module_4A.ipynb) | Making maps with PyGMT | Earthquake catalogue |\n",
    "| [4B](ESCI451_Module_4B.ipynb) | Gridded data and vectors | Ashfall data and GNSS |\n",
    "| **[5](ESCI451_Module_5_EXTRAS.ipynb)** | **Extras: b-values and fourier analysis** | **Earthquake catalogue andHistorical Temperature data** |\n",
    "\n",
    "The content may change in response to students' questions or current events. Each of the four modules has been designed to take about three hours, with a short break between each of the two parts.\n",
    "\n",
    "---\n",
    "\n",
    "## Extras\n",
    "\n",
    "Kia ora you keen bean! This notebook contains extra examples that go beyond some of the basics given in the other notebooks. Some of them are stand-alone examples relevent to specific topics, and others are extended examples of particular nuanced Python things. **You don't need to work through this notebook for ESCI451**, but you might find some of these examples useful in the future.\n",
    "\n",
    "The examples are:\n",
    "1. Using pandas to compute time-variations in b-values;\n",
    "2. Basic Fourier analysis;\n",
    "3. ... (we may add more!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798cf98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Computing b-values\n",
    "\n",
    "Earthquake generally follow a Gutenberg-Richter relationship, where the logarithm of the cumulative number of earthquakes above a given magnitude is proportional to the magnitude:\n",
    "\\begin{equation}\n",
    "    \\log_{10}{N} = a - bM\n",
    "\\end{equation}\n",
    "where *M* is magnitude, *N* is the number of events with magnitude >= *M*, and *a* and *b* are constants. This is a nice simple straight-line equation with offset from the origin given by *a* and the gradient by *b*.\n",
    "\n",
    "Some studies (for example, [Nuannin et al., 2005](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005GL022679)) have found variations in b-value with time and space, and related this to changes in stress.  Lets see if we can:\n",
    "1. Calculate the b-value for our dataset;\n",
    "2. Do some sliding-window fu to get at b-value variations in time.\n",
    "\n",
    "To kick us off, note that in our analysis we are going to miss one fundamental thing which means that everything we do is wrong.  That thing is catalogue completeness, upon which our b-value calculations depend. To show that completeness, and have a first pass at computing b-values, lets look at a cumulative distribution of earthquake magnitudes.\n",
    "\n",
    "### Plotting cumulative distributions\n",
    "\n",
    "We want an inverse cumulative plot of magnitudes. We can do this with matplotlib's `hist` by setting the `cumulative` argument to `-1`, and the `density` argument set to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a952c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from helpers.get_data import get_geonet_quakes\n",
    "\n",
    "kaikoura = get_geonet_quakes(\n",
    "    min_latitude=-43.12, max_latitude=-41.15,\n",
    "    min_longitude=172.37, max_longitude=174.95,\n",
    "    start_time=datetime.datetime(2010, 1, 1),\n",
    "    min_magnitude=2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(\n",
    "    kaikoura[\"magnitude\"], bins=len(kaikoura),\n",
    "    histtype=\"step\", density=True, log=True,\n",
    "    cumulative=-1)\n",
    "ax.set_xlabel(\"Magnitude\")\n",
    "ax.set_ylabel(\"Cumulative density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f2fe2",
   "metadata": {},
   "source": [
    "Looks pretty straight on a log-normal plot, as we would expect from the Gutenberg-Richter law.  However, somewhere between M 2 and 3 it stops being straight.  We assume that our catalogue completeness is somewhere in here.  This means that we think that, if we could detect and catalogue all the earthquakes all the way down to the tiny earthquakes, we would continue seeing this log-normal relationship. So, we assume that below our magnitude of completeness ($M_C$) we are missing earthquakes.  This seems reasonable, as earthquakes get smaller they get much harder to detect simply because their amplitudes are greatly reduced.\n",
    "\n",
    "Lets *assume* our catalogue is complete to $M_C=2.5$ and try and fit a straight line to our cumulative-density plot.\n",
    "\n",
    "First we will count how many times each magnitude appears in our dataset, we will use a handy object in Pythons native `collections` library, called `Counter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5643c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counted_magnitudes = Counter(kaikoura[\"magnitude\"])\n",
    "\n",
    "# Print the most common 10 magnitudes\n",
    "print(counted_magnitudes.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369af05",
   "metadata": {},
   "source": [
    "Cool, that gives us a list of the magnitude and the number of occurrences of that magnitude.  What we actually want is magnitudes and the number of occurrences of that magnitude and *any magnitude above that magnitude*.  To do that we will:\n",
    "1. create a unique set of all the magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = set(kaikoura[\"magnitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a9108",
   "metadata": {},
   "source": [
    "2. Make a sorted `list` from this set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = sorted(list(magnitudes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5220ba",
   "metadata": {},
   "source": [
    "3. Remove all magnitudes below our completeness using a [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) (because they are handy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = [m for m in magnitudes if m >= 2.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c9474",
   "metadata": {},
   "source": [
    "4. Initialise an empty array in which we will put the cumulative density function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ed6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "density = np.zeros(len(magnitudes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7445d971",
   "metadata": {},
   "source": [
    "5. Loop through the magnitudes from largest to smallest and add the number of occurrences of that magnitude to the total occurrences of the previous magnitude bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f0d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "density[0] = counted_magnitudes[magnitudes[0]]\n",
    "for i, magnitude in enumerate(magnitudes[1:]):\n",
    "    density[i + 1] = density[i] + counted_magnitudes[magnitude]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8d84e",
   "metadata": {},
   "source": [
    "Lets check that that looks okay by plotting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f5cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(magnitudes, density)\n",
    "ax.set_ylabel(\"Cumulative density\")\n",
    "ax.set_xlabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8270a4a",
   "metadata": {},
   "source": [
    "Looks good! Now lets try and fit a line to it.  We can use `numpy`'s solvers to do this. Because this is a nice\n",
    "simple equation we will use the [numpy.polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a01490",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients, residual, rank, singular_values, rcondition = np.polyfit(\n",
    "    magnitudes, np.log10(density), deg=1, full=True)\n",
    "b, a = coefficients\n",
    "print(f\"a={a:.2f}, b={b:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a66134",
   "metadata": {},
   "source": [
    "b is usually close to 1 (note that the gradient calculated above is negative, which is already taken care of in the Gutenberg-Richter law). \n",
    "\n",
    "Lets estimate the density from our calculated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71524cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make our lives easier we will convert our magnitudes to a numpy array:\n",
    "magnitudes = np.array(magnitudes)\n",
    "estimated_density = 10 ** (a + (magnitudes * b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6385e0d0",
   "metadata": {},
   "source": [
    "Right, lets see if it fits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(\n",
    "    magnitudes, density, marker=\"+\", linestyle=\"None\",\n",
    "    label=\"Data\")\n",
    "ax.semilogy(magnitudes, estimated_density, label=\"Model\")\n",
    "ax.set_ylabel(\"Cumulative density\")\n",
    "ax.set_xlabel(\"Magnitude\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964edf7",
   "metadata": {},
   "source": [
    "Note that because we specified `full=True` in our call to `polyfit`, we were returned a range of metrics about how well-fitted our data were.  The easiest one of those to understand is the residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44ba00",
   "metadata": {},
   "source": [
    "This is a measure of the misfit between our model and our data.\n",
    "\n",
    "Lets build a simple function to do this with the aim of applying this to distinct time-chunks of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7786d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_b_value(magnitudes, completeness_magnitude=2.5):\n",
    "    \"\"\"\n",
    "    Calculate the b-value for a range of magnitudes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    magnitudes\n",
    "        List or array of magnitudes\n",
    "    completeness_magnitude\n",
    "        Magnitude of completeness for the dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    b-value\n",
    "    \"\"\"\n",
    "    counted_magnitudes = Counter(magnitudes)\n",
    "    magnitudes = sorted(list(set(magnitudes)), reverse=True)\n",
    "    magnitudes = np.array(magnitudes)\n",
    "    # Remove magnitudes less than completess\n",
    "    magnitudes = magnitudes[magnitudes >= completeness_magnitude]\n",
    "    # Calculate density\n",
    "    density = np.zeros(len(magnitudes))\n",
    "    density[0] = counted_magnitudes[magnitudes[0]]\n",
    "    for i, magnitude in enumerate(magnitudes[1:]):\n",
    "        density[i + 1] = density[i] + counted_magnitudes[magnitude]\n",
    "    coefficients, residual, rank, singular_values, rcondition = np.polyfit(\n",
    "        magnitudes, np.log10(density), deg=1, full=True)\n",
    "    b, a = coefficients\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a56d78",
   "metadata": {},
   "source": [
    "Lets check that we get the same b-value as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723541cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = calc_b_value(kaikoura[\"magnitude\"])\n",
    "print(f\"b={b:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4cbb2",
   "metadata": {},
   "source": [
    "### Rolling windows with Pandas\n",
    "\n",
    "Pandas has neat ways of doing rolling windows.  We will use this to do two things:\n",
    "1. Calculate the median date for every 2000 earthquakes;\n",
    "2. Calculate the b-value for every 2000 earthquakes.\n",
    "\n",
    "We will then plot these and see if we see any variations.\n",
    "\n",
    "To calculate the median date we will:\n",
    "1. sort the dataframe by `\"origintime\"`\n",
    "2. Extract just the `\"origintime\"` and `\"magnitude\"` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb684686",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2000\n",
    "\n",
    "kaikoura = kaikoura.sort_values(by=[\"origintime\"], ignore_index=True)\n",
    "magnitude_times = pd.concat(\n",
    "    [kaikoura[\"origintime\"], kaikoura[\"magnitude\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f899e",
   "metadata": {},
   "source": [
    "3. Make a new column containing the seconds since the first event - pandas doesn't have a simple way to calculate the median of a range of datetimes, so we will change to working in seconds since a reference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_offset = (magnitude_times.origintime -\n",
    "                  magnitude_times.origintime[0]).dt.total_seconds()\n",
    "magnitude_times = magnitude_times.merge(\n",
    "    seconds_offset.rename(\"seconds_offset\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85697c2f",
   "metadata": {},
   "source": [
    "4. Compute the rolling median of the seconds_offset column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bced772",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_median = magnitude_times.seconds_offset.rolling(window_size).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e60774",
   "metadata": {},
   "source": [
    "5. Convert this column to timedelta objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d92f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_median = pd.to_timedelta(window_median, unit=\"S\")  # Unit is seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29274217",
   "metadata": {},
   "source": [
    "6. Add the reference time to these to get back to real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebd708",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_median += magnitude_times.origintime[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee26ac",
   "metadata": {},
   "source": [
    "7. Put this into the dataframe as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4670b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_times = magnitude_times.merge(\n",
    "    window_median.rename(\"window_median\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b0c7a",
   "metadata": {},
   "source": [
    "### Computing moving window b-values\n",
    "\n",
    "Computing the moving b-value is a little simpler to write, but slower to run.  We will use the function we wrote above and pandas `.rolling().apply(func)` chained method to apply our custom `func` to our column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_values = magnitude_times.magnitude.rolling(window_size).apply(calc_b_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c242299a",
   "metadata": {},
   "source": [
    "Lets quickly convert those from gradients to b-values by multiplying by -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b852a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_values *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af7d26",
   "metadata": {},
   "source": [
    "Now we can put those back into the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_times = magnitude_times.merge(\n",
    "    b_values.rename(\"b_value\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f00263",
   "metadata": {},
   "source": [
    "### Plotting the results\n",
    "\n",
    "Now lets plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = magnitude_times.plot(x=\"window_median\", y=\"b_value\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e70057",
   "metadata": {},
   "source": [
    "### What next?\n",
    "\n",
    "There are some pretty impressive variations there! In particular there are strong variations in 2013 and 2016, right around when the Cook Strait and Kaikoura earthquakes happened. I wonder if there is anything in that...? **before we get ahead of ourselves**, we missed some key things here that mean that this result is not interpretable:\n",
    "1. Not all magnitudes are equal, and we were just using GeoNet's summary magnitude;\n",
    "2. We fixed the magnitude of completeness when in reality completeness depends on a range of factors and is time-varying;\n",
    "3. We haven't taken spatial variations into account - we have looked at quite a large region here.\n",
    "\n",
    "We could get around those factors though and extend our rolling window to compute completeness alongside b-value. Potential student project...?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8811f45b",
   "metadata": {},
   "source": [
    "**Exercise:** Using pandas rolling windows, find the mean earthquake location for every window we used above. You will need to compute the rolling mean for latitude, longitude and depth.  Make three plots to show how latitude, longitude and depth vary with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25752891",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Fourier analysis using SciPy\n",
    "\n",
    "We won't go into much detail about how different signal processing tools, not least the workhorse that is the the Fourier Transform, can be applied to time series data in Python. However, the following code illustrates the way in which the obvious annual cyclicity in the CliFlo temperatures can be extracted and illustrated using tools from [SciPy](https://www.scipy.org/). We have defined a function that uses the Fast Fourier Transfrom to isolate the annual signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e06b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26097732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parser(date): return dt.strptime(date, '%Y%m%d:%H%M')\n",
    "\n",
    "\n",
    "T0 = pd.read_csv(\n",
    "    'data/1939-1989-surface-temperature-CliFlo.csv',\n",
    "    parse_dates=['Date'], date_parser=parser, index_col='Date')\n",
    "T10 = pd.read_csv(\n",
    "    'data/1939-1989-10cm-temperature-CliFlo.csv',\n",
    "    parse_dates=['Date'], date_parser=parser, index_col='Date')\n",
    "T30 = pd.read_csv(\n",
    "    'data/1939-1989-30cm-temperature-CliFlo.csv',\n",
    "    parse_dates=['Date'], date_parser=parser, index_col='Date')\n",
    "T100 = pd.read_csv(\n",
    "    'data/1939-1989-100cm-temperature-CliFlo.csv',\n",
    "    parse_dates=['Date'], date_parser=parser, index_col='Date')\n",
    "\n",
    "T0['Temperature'] = T0.apply(lambda df: (df['Tmax'] + df['Tmin']) / 2, axis=1)\n",
    "\n",
    "merged1 = pd.merge(left=T0, right=T10, how='inner',\n",
    "                   left_on='Date', right_on='Date')\n",
    "merged1.rename(columns={'Temperature_x': 'T0',\n",
    "               'Temperature_y': 'T10'}, inplace=True)\n",
    "merged2 = pd.merge(left=merged1, right=T30, how='inner',\n",
    "                   left_index=True, right_index=True)\n",
    "merged2.rename(columns={'Temperature': 'T30'}, inplace=True)\n",
    "temperatures = pd.merge(left=merged2, right=T100,\n",
    "                        how='inner', left_index=True, right_index=True)\n",
    "temperatures.rename(columns={'Temperature': 'T100'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal, stats, fftpack\n",
    "import numpy as np\n",
    "# https://ipython-books.github.io/101-analyzing-the-frequency-components-of-a-signal-with-a-fast-fourier-transform/\n",
    "\n",
    "\n",
    "def annular_component(df, column):\n",
    "    '''\n",
    "    Analyse spectrum and return reconstruction \n",
    "    containing frequencies lower than annual\n",
    "    '''\n",
    "    times = df[column].to_numpy()\n",
    "\n",
    "    freq_amp = fftpack.fft(times)\n",
    "    power_spectral_density = np.abs(freq_amp) ** 2\n",
    "    freqs = fftpack.fftfreq(len(power_spectral_density), 1. / 365)\n",
    "    i = freqs > 0\n",
    "    freq_amp_copy = freq_amp.copy()\n",
    "    freq_amp_copy[np.abs(freqs) > 1.01] = 0\n",
    "\n",
    "    annular = np.real(fftpack.ifft(freq_amp_copy))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "    ax[0].plot(freqs[i], 10 * np.log10(power_spectral_density[i]), 'o-')\n",
    "    ax[0].set_xlim(0, 5)\n",
    "    ax[0].set_xlabel('Frequency (1/year)')\n",
    "    ax[0].grid()\n",
    "    ax[0].set_ylabel('PSD (dB)')\n",
    "\n",
    "    ax[1].plot(df.index, times, label='Raw signal', color='lightgray')\n",
    "    ax[1].plot(df.index, annular, label='Annual component',\n",
    "               color='black', linewidth=3)\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel('Date')\n",
    "    ax[1].set_ylabel('Temperature (C)')\n",
    "\n",
    "    return annular, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1bd74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annular, fig = annular_component(temperatures, 'T0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0dbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal, stats, fftpack\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate.html#scipy.signal.correlate\n",
    "\n",
    "t0 = stats.zscore(temperatures['T0'].rolling(\n",
    "    window=3).mean().to_numpy(na_value=0))\n",
    "t10 = stats.zscore(temperatures['T10'].rolling(\n",
    "    window=7).mean().to_numpy(na_value=0))\n",
    "t30 = stats.zscore(temperatures['T30'].rolling(\n",
    "    window=7).mean().to_numpy(na_value=0))\n",
    "t100 = stats.zscore(temperatures['T100'].rolling(\n",
    "    window=3).mean().to_numpy(na_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f963c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = plt.xcorr(t10, t100, maxlags=365)\n",
    "# https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.xcorr.html\n",
    "l = corr[0]\n",
    "c = corr[1]\n",
    "print(l[np.argmax(c)])\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(l, c)\n",
    "ax.plot(l[np.argmax(c)], np.max(c), 'ro')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
