{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python for Earth Scientists\n",
    "\n",
    "These notebooks have been developed by Calum Chamberlain, Finnigan Illsley-Kemp and John Townend at [Victoria University of Wellington-Te Herenga Waka](https://www.wgtn.ac.nz) for use by Earth Science graduate students. \n",
    "\n",
    "The notebooks cover material that we think will be of particular benefit to those students with little or no previous experience of computer-based data analysis. We presume very little background in command-line or code-based computing, and have compiled this material with an emphasis on general tasks that a grad student might encounter on a daily basis. \n",
    "\n",
    "In 2022, this material will be delivered at the start of Trimester 1 in conjunction with [ESCI451 Active Earth](https://www.wgtn.ac.nz/courses/esci/451/2022/offering?crn=32176). Space and pandemic alert levels permitting, interested students not enrolled in ESCI451 are encouraged to come along too but please contact Calum, Finn, or John first.\n",
    "\n",
    "| Notebook | Contents | Data |\n",
    "| --- | --- | --- |\n",
    "| [1A](ESCI451_Module_1A.ipynb) | Introduction to programming, Python, and Jupyter notebooks | - |\n",
    "| [1B](ESCI451_Module_1B.ipynb) | Basic data types and variables, getting data, and plotting with Matplotlib | Geodetic positions |\n",
    "| [2A](ESCI451_Module_2A.ipynb) | More complex plotting, introduction to Numpy | Geodetic positions; DFDP-2B temperatures |\n",
    "| [2B](ESCI451_Module_2B.ipynb) | Using Pandas to load, peruse and plot data | Earthquake catalogue  |\n",
    "| **[3A](ESCI451_Module_3A.ipynb)** | **Working with Pandas dataframes** | **Geochemical data set; GNSS data** |\n",
    "| [3B](ESCI451_Module_3B.ipynb) | Simple time series analysis using Pandas | Historical temperature records |\n",
    "| [4A](ESCI451_Module_4A.ipynb) | Making maps with PyGMT | Earthquake catalogue |\n",
    "| [4B](ESCI451_Module_4B.ipynb) | Gridded data and vectors | Ashfall data and GNSS |\n",
    "\n",
    "The content may change in response to students' questions or current events. Each of the four modules has been designed to take about three hours, with a short break between each of the two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook\n",
    "\n",
    "1. More with Pandas dataframes\n",
    "   - Loading and summarising another dataset\n",
    "   - Working with subplots\n",
    "   - Regression\n",
    "   - Dates and times and datetimes\n",
    "   - Querying a dataframe using a function\n",
    "   - Applying functions to a dataframe\n",
    "2. Extension: more sophisticated analysis of a big dataset\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and summarising another dataset\n",
    "\n",
    "Let's recap where we got to with Pandas in the previous notebook. We learnt how to obtain data from GeoNet and store it in .csv file and a Pandas dataframe, which we could then inspect and plot in various ways. \n",
    "\n",
    "We'll start this notebook by reviewing some of the ways in which we can inspect, describe and plot Pandas dataframes, and then focus on working with dates and times. The first step is to reload the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a geochemical data set obtained by a PhD student, Elliot Swallow, for volcanic rocks from Huckleberry Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geochem = pd.read_csv(\n",
    "    \"data/Edited Swallow et al J Petrol data for plotting.csv\",\n",
    "    index_col=\"Sample\")\n",
    "print(geochem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing a dataframe out gives us a rough look at the data and we've seen before how we can compute various statistics to explore the dataframe piece by piece. But there's another way of getting a nicely-formatted, informative summary of the whole dataframe and that is to use the `.describe()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geochem.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How cool is that?!\n",
    "\n",
    "## Working with subplots\n",
    "\n",
    "What we can immediately see is that this dataset contains a lot of geochemical parameters (38!) and if we want to discover how they relate to each other we're going to have to do some more plotting. Ideally, it would be interesting to plot each parameter against every other one but this will get pretty overwhelming so we'll focus on a few interesting ones.\n",
    "\n",
    "To make our plots, we'll use the `plt.subplots()` syntax we've encountered previously, but which we haven't yet used to its full extent. As the name suggests, subplots let you make multiple plots in one. \n",
    "\n",
    "To start with, let's plot the first four major elements in the table ($TiO_2$, $Al_2O_3$, $Fe_2O_3 (T)$, $MnO$) against $Si0_2$.\n",
    "\n",
    "To do this we will need four axes.  We can make four axes using the `plt.subplots` command.  Lets get help for that function.  You can get to the docs for a given function by typing `function?`.  Run the cell below and you should see a pop-up of the docs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments that we care about at the moment at:\n",
    "- `nrows`, the number of rows of subplots\n",
    "- `ncols`, the number of columns of subplots\n",
    "- `sharex`, whether axes should have the same x-axis.  We will be having $SiO_2$ on all the x-axes, so we will set this to be `True`.\n",
    "\n",
    "Lets make a 2x2 grid of subplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we Ä«ncluded `sharex=True` in our command, which will ensure that all four panels have the same x-axis.\n",
    "\n",
    "This returned two things as usual, `fig`: the `figure` containing all our axes, and `ax`, which is a list of the subplot axes.  We can specify which axes we want to use for each graph by remembering that Python counts from zero and noting that _the axes are indexed by row and then by column_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True)\n",
    "for column in range(2):\n",
    "    for row in range(2):\n",
    "        ax[row][column].set_title(f\"ax[{row}][{column}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, let's plot actual data using two loops to run through the parameters and axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True)\n",
    "\n",
    "elements = [[\"TiO2 (wt%)\", \"Al2O3 (wt%)\"], [\"Fe2O3 (T) (wt%)\", \"MnO (wt%)\"]]\n",
    "for column in range(2):\n",
    "    for row in range(2):\n",
    "        element_name = elements[row][column]\n",
    "        ax[row][column].scatter(geochem[\"SiO2 (wt%)\"], geochem[element_name])\n",
    "        ax[row][column].set_ylabel(element_name)\n",
    "        ax[row][column].set_xlabel(\"$SiO_2$ (wt%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y-axis labels are getting in the way a bit but we can easily avoid this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True)\n",
    "\n",
    "elements = [[\"TiO2 (wt%)\", \"Al2O3 (wt%)\"], [\"Fe2O3 (T) (wt%)\", \"MnO (wt%)\"]]\n",
    "for column in range(2):\n",
    "    for row in range(2):\n",
    "        element_name = elements[row][column]\n",
    "        ax[row][column].scatter(geochem[\"SiO2 (wt%)\"], geochem[element_name])\n",
    "        ax[row][column].set_ylabel(element_name)\n",
    "        ax[row][column].set_xlabel(\"$SiO_2$ (wt%)\")\n",
    "        if column == 1:\n",
    "            ax[row][column].yaxis.tick_right()\n",
    "            ax[row][column].yaxis.set_label_position(\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Try writing some code to plot the four parameters of interest against $SiO_2$ in a 1x4 subplot layout with a common y-axis ($SiO_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We can infer from the plots above that $Fe_2O_3$ and $TiO_2$ exhibit very similar behaviour with regard to $SiO_2$. Sure enough, we find when we plot one against the other that they have are highly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geochem.plot(x=\"Fe2O3 (T) (wt%)\", y=\"TiO2 (wt%)\", kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these two parameters to briefly explore linear regression. There are myriad ways to do this in Python but perhaps the simplest to start with is numpy's [polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model, covariance = np.polyfit(\n",
    "    geochem['Fe2O3 (T) (wt%)'], geochem['TiO2 (wt%)'], 1, cov=True)\n",
    "print(model)\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've done is fit a linear model of the form $y=ax+b$ to the Fe (x) and Ti (y) oxide dat, using least-shokies. We could have fit a quadratic model by simply changing the \"1\" at the end of the command to \"2\".\n",
    "\n",
    "We can define a simple function to construct a function from the model parameters that, when re-applied to the Fe data, will return the predicted Ti measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_func(x):\n",
    "    return (x * model[0]) + model[1]\n",
    "\n",
    "\n",
    "Ti_predicted = geochem['Fe2O3 (T) (wt%)'].apply(pred_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results to see what we've found and whether it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(geochem['Fe2O3 (T) (wt%)'], geochem['TiO2 (wt%)'],\n",
    "        'o', color='red', label='Measurements')\n",
    "ax.plot(geochem['Fe2O3 (T) (wt%)'], Ti_predicted, label='Linear model',\n",
    "        linestyle='dotted', linewidth=0.5, color='red')\n",
    "ax.set_xlabel(\"Fe2O3 (T) (wt%)\")\n",
    "ax.set_ylabel(\"TiO2 (wt%)\")\n",
    "ax.set_title(\"An example of linear regression with numpy\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on dates and times and datetimes\n",
    "\n",
    "In the previous notebook, we made our GeoNet query using `str` objects for the start and end-time arguments, but Python has nice native ways of working with dates and times. These do useful things like cope with leap-years, allow you to add seconds (or other time units) to dates and times, and allow you to format dates and times as `str` objects.\n",
    "\n",
    "We should switch from giving `str`s to giving `datetime`s.  `datetime` objects come from Python's native `datetime` library, and include a handy `.strftime` method which is literally string-format-time.  We will use that to make the correctly formatted string for our query.  The query requires something of the format:\n",
    "\n",
    "> year-month-dayThour:minute:second\n",
    "\n",
    "In `datetime` speak the format string for that is:\n",
    "\n",
    "> `%Y-%m-%dT%H:%M:%S`\n",
    "\n",
    "- `%Y` is a four-digit year (e.g.: ..., 2018, 2019, 2020, ...)\n",
    "- `%m` is a two-digit month (e.g.: 01, 02, 03, 04, 05, 06, 07, 08, 09, 10, 11, 12)\n",
    "- `%d` is a two-digit day (zero-padded as months are, e.g.: 01, 02, ... 28, 29, 30, 31)\n",
    "- `T` is just a letter, anything not preceded by a `%` sign is interpreted as a `str`\n",
    "- `%H` is a two-digit hour (as above)\n",
    "- `%M` is a two-digit minute (as above)\n",
    "- `%S` is a two-digit second (as above).\n",
    "\n",
    "Other formatters, for things like day of the week, month as a word, julian-day, milliseconds, etc. can be found in the [offical docs](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes).\n",
    "\n",
    "Lets see how we could format a `datetime`.  To start we need to make a `datetime` object, we can provide arguments of the year, month, day, hour, minute, second, (millisecond) to make one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime is the module, datetime.datetime is the object itself\n",
    "test_time = datetime.datetime(2020, 1, 10, 12, 43, 10)\n",
    "print(test_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets format the string the way that we want it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_time.strftime(\"%Y-%m-%dT%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** format the `datetime` object as \"year/month/day hour:minute:seconds\" and then try some other formats using the information listed in the [offical docs](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how it all works, lets put it together into a function with some useful\n",
    "arguments that we can query.\n",
    "\n",
    "## Querying a dataframe using a function\n",
    "\n",
    "We will set some default values for our arguments, so that we do not always have to\n",
    "specify every argument.  These defaults are given in the function definition as:\n",
    "\n",
    "```python\n",
    "def function(argument=value, ...):\n",
    "    contents_of_function\n",
    "```\n",
    "where `argument` is the argument name, and `value` is the default value for that argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geonet_quakes(\n",
    "    min_latitude=-49.0, max_latitude=-40.0,\n",
    "    min_longitude=164.0, max_longitude=182.0,\n",
    "    min_magnitude=1, max_magnitude=9.0,\n",
    "    min_depth=0.0, max_depth=500.0,\n",
    "    start_time=datetime.datetime(1960, 1, 1),\n",
    "    end_time=datetime.datetime(2020, 1, 1),\n",
    "):\n",
    "    \"\"\"\n",
    "    Get a dataframe of the eatrhquakes in the GeoNet catalogue.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_latitude\n",
    "        Minimum latitude in degrees for search\n",
    "    max_latitude\n",
    "        Maximum latitude in degrees for search\n",
    "    min_longitude\n",
    "        Minimum longitude in degrees for search\n",
    "    max_longitude\n",
    "        Maximum longitude in degrees for search\n",
    "    min_depth\n",
    "        Minimum depth in km for search\n",
    "    max_depth\n",
    "        Maximum depth in km for search\n",
    "    min_magnitude\n",
    "        Minimum magnitude for search\n",
    "    max_magnitude\n",
    "        Maximum magnitude for search\n",
    "    start_time\n",
    "        Start date and time for search\n",
    "    end_time\n",
    "        End date and time for search\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DateFrame of resulting events\n",
    "    \"\"\"\n",
    "    # Convert start_time and end_time to strings\n",
    "    start_time = start_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    end_time = end_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    # Use the more efficient f-string formatting\n",
    "    query_string = (\n",
    "        \"https://quakesearch.geonet.org.nz/csv?bbox=\"\n",
    "        f\"{min_longitude},{min_latitude},{max_longitude},\"\n",
    "        f\"{max_latitude}&minmag={min_magnitude}\"\n",
    "        f\"&maxmag={max_magnitude}&mindepth={min_depth}\"\n",
    "        f\"&maxdepth={max_depth}&startdate={start_time}\"\n",
    "        f\"&enddate={end_time}\")\n",
    "    print(f\"Using query: {query_string}\")\n",
    "    response = requests.get(query_string)\n",
    "    with open(\"data/earthquakes.csv\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    earthquakes = pd.read_csv(\n",
    "        \"data/earthquakes.csv\",\n",
    "        parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "        dtype={\"publicid\": str})\n",
    "    earthquakes = earthquakes.rename(\n",
    "        columns={\" magnitude\": \"magnitude\",\n",
    "                 \" latitude\": \"latitude\",\n",
    "                 \" depth\": \"depth\"})\n",
    "    return earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly run this function to get the data.  There won't be any output.  Note that I didn't\n",
    "provide these data in the repository because:\n",
    "1. I don't have permission to re-distribute the data and,\n",
    "2. this dataset gets updated frequently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes = get_geonet_quakes(\n",
    "    start_time=datetime.datetime(2015, 1, 1), min_magnitude=3)\n",
    "\n",
    "print(earthquakes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do something useful: applying functions to a dataframe\n",
    "\n",
    "We saw in the previous notebook how we could sort and slice a dataframe in various ways.Let's look at\n",
    "an example of doing some actual calculations with a dataframe.  We will use some of what we have\n",
    "learnt to calculate the occurance rate of earthquakes within a region.  In this case\n",
    "we will take the region around the top of South Island, containing the faults that ruptured\n",
    "in the Kaikoura earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaikoura = get_geonet_quakes(\n",
    "    min_latitude=-43.12, max_latitude=-41.15,\n",
    "    min_longitude=172.37, max_longitude=174.95,\n",
    "    start_time=datetime.datetime(2010, 1, 1),\n",
    "    min_magnitude=2)\n",
    "# Rename those columns\n",
    "kaikoura = kaikoura.rename(\n",
    "    columns={\" magnitude\": \"magnitude\",\n",
    "             \" latitude\": \"latitude\",\n",
    "             \" depth\": \"depth\"})\n",
    "\n",
    "print(f\"Downloaded {len(kaikoura)} earthquakes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earthquake rate is the number of earthquakes per unit time.  For a dataset like this we can calculate\n",
    "rate as 1 over the inter-event time.\n",
    "\n",
    "First we need to sort by origin time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaikoura = kaikoura.sort_values(by=[\"origintime\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to calculate the time between each successive earthquake.  We can do this by\n",
    "taking the `origintime` column away from a one-sample shifted version of the `origintime` column,\n",
    "much like we did for calculating the temperature gradient in the DFDP drillhole. In Pandas we\n",
    "can do this quickly using the `.diff` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_event_time = kaikoura[\"origintime\"].diff()\n",
    "print(inter_event_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column is in `timedelta` format. Lets convert it to seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_event_time = inter_event_time.dt.total_seconds()\n",
    "print(inter_event_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rate is simply 1 / `inter_event_time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 1 / inter_event_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add this column into the dataframe and call it `\"rate\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaikoura['rate'] = rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot rate as a function of time. Remember that the time values we use here are the\n",
    "event origin-times, which do not directly represent the time of the rate calculated, which\n",
    "is an average between events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = kaikoura.plot(x=\"origintime\", y=\"rate\")\n",
    "ax.set_ylabel(\"Instantaneous Rate (earthquakes per second)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is lots more that you can do with dataframes.  Pandas is in heavy use in the datascience\n",
    "world, and allows you to quickly explore datasets.  Hopefully you will find it useful at some\n",
    "point in your Geoscience career.\n",
    "\n",
    "As a final thing, you can save your dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaikoura.to_csv(\"data/kaikoura.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "That covers *some* of the basics of `dataframes` in pandas.  You should be able to use them to replace most of what you would have done with spreadsheets to allow you to work in a more programatic and reproducible way. We also demonstrated some of the basics of fitting simple polynomials to data.  Note that numpy's `polyfit` function isn't limited to 1st order polynomials, the sky is the limit!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: more sophisticated analysis of a dataframe\n",
    "\n",
    "### Extension: calculating velocities and strain from GNSS data\n",
    "\n",
    "You might remember from ESCI203 that we can use GNSS (GPS is one famly of Global Navigation Satellite System or GNSS) to calculate velocities, as well as strain between places. In ESCI203 you calculated this by hand, fitting lines to data, but we can do so much better! In this extension we will:\n",
    "\n",
    "1. Calculate the average velocities for a few continuous GNSS sites in NZ;\n",
    "2. Do some sliding-window fun to work out the relative velocity between these sites and how that velocity varies with time.\n",
    "\n",
    "To start with we need to get some data. We will consider two sites on either side of the Alpine Fault: [HOKI](https://www.geonet.org.nz/data/network/mark/HOKI) and [MTJO](https://www.geonet.org.nz/data/network/mark/MTJO). We can use our helper function from before to get those data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.get_data import get_gnss_for_station\n",
    "\n",
    "starttime, endtime = datetime.datetime(2001, 1, 1), datetime.datetime(2022, 1, 1)\n",
    "\n",
    "hoki = get_gnss_for_station(\n",
    "    \"HOKI\", starttime=starttime, endtime=endtime)\n",
    "mtjo = get_gnss_for_station(\n",
    "    \"MTJO\", starttime=starttime, endtime=endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 5), sharex=True)\n",
    "\n",
    "for component in [\"north\", \"east\", \"up\"]:\n",
    "    axes[0].plot(hoki[\"time\"], hoki[component], label=component)\n",
    "    axes[1].plot(mtjo[\"time\"], mtjo[component], label=component)\n",
    "for ax, station in zip(axes, [\"HOKI\", \"MTJO\"]):\n",
    "    ax.set_xlabel(\"Sample time (UTC)\")\n",
    "    ax.set_ylabel(\"Offset in mm\")\n",
    "    ax.set_title(f\"GNSS offsets for {station}\")\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have those data we need to combine them into a dataframe to use some sliding windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoki_df = pd.DataFrame.from_dict(hoki)\n",
    "mtjo_df = pd.DataFrame.from_dict(mtjo)\n",
    "\n",
    "hoki_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next notebook covers merging dataframes in more detail, but for now we will just make a merged dataframe based on using the date as the index. Note that the merge below drops values that don't have matching times between the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoki_df = pd.DataFrame.from_dict(hoki)\n",
    "mtjo_df = pd.DataFrame.from_dict(mtjo)\n",
    "\n",
    "hoki_df.set_index(\"time\", inplace=True)\n",
    "mtjo_df.set_index(\"time\", inplace=True)\n",
    "\n",
    "af_data = pd.merge(\n",
    "    left=hoki_df, right=mtjo_df, how='inner', \n",
    "    left_index=True, right_index=True,\n",
    "    suffixes=(\"_hoki\", \"_mtjo\"))\n",
    "\n",
    "af_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing velocity\n",
    "\n",
    "We know that velocity is just $\\frac{distance}{time}$. We can fairly simply work out the daily velocity by calculating the difference between two data points and then dividing by the time between. To exemplify this we are going to focus on the east component of motion and make a simpler dataframe that we can play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_east = pd.merge(\n",
    "    left=af_data.east_hoki, right=af_data.east_mtjo,\n",
    "    left_index=True, right_index=True)\n",
    "\n",
    "print(af_east)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_east = pd.merge(\n",
    "    left=af_data.east_hoki, right=af_data.east_mtjo,\n",
    "    left_index=True, right_index=True)\n",
    "\n",
    "# First work out the sample interval - this isn't always consistent!\n",
    "delta = (af_east.index[1:] - af_east.index[0:-1]).total_seconds() / (365 * 86400)\n",
    "\n",
    "# Compute velocity\n",
    "east_hoki_vel = (af_east.east_hoki[1:].to_numpy() - af_east.east_hoki[0:-1].to_numpy()) / delta\n",
    "east_mtjo_vel = (af_east.east_mtjo[1:].to_numpy() - af_east.east_mtjo[0:-1].to_numpy()) / delta\n",
    "\n",
    "# Put back into the dataframe\n",
    "af_east[\"east_hoki_vel\"] = pd.Series(east_hoki_vel, index=af_east.index[1:])\n",
    "af_east[\"east_mtjo_vel\"] = pd.Series(east_mtjo_vel, index=af_east.index[1:])\n",
    "\n",
    "print(af_east)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = af_east[[\"east_hoki_vel\", \"east_mtjo_vel\"]].plot()\n",
    "ax.set_ylabel(\"Velocity (mm/day)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Pandas .rolling()\" align=\"right\" style=\"width:30%\" src=\"https://64.media.tumblr.com/tumblr_ma8zvcFa6z1r38hk2o1_500.jpg\">\n",
    "\n",
    "Nice! But it looks really noisy, and it is hard to see what in actually plotted. One thing that we might want to do to reduce the noise in the data is to look at average velocities over some window length. Pandas has a really useful method to do this by creating \"rolling windows\" using the (you guessed it!) [`.rolling()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html) method.\n",
    "\n",
    "The rolling method breaks the data down into overlapping chunks of the length specified by the `window` argument (the first argument below). Pandas is also clever enough to take window lengths as time-steps, which is really handy for our irregularly sampled data: instead of having to take n datapoint, which might be a week, or might be a month depending on when the samples come in, we can specify that our window is n days (or seconds, weeks, years...) long. To do that below I have set a window length of 7 days (with the `d` noting that this should be days). \n",
    "\n",
    "To ensure that only chunks with the full set of 7 days of samples are included I have set the `min_periods` argument to 7 - you could set this to less if you want to include non-full windows.\n",
    "\n",
    "I have then applied the `.median()` method to compute the median of each moving window. Finally I changed the `alpha` when plotting to make the lines transparent so that we can see the data overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = af_east.rolling(\"7d\", min_periods=7).median()[[\"east_hoki_vel\", \"east_mtjo_vel\"]].plot(alpha=0.5)\n",
    "ax.set_ylabel(\"Velocity (mm/day)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all very well, but velocity is a vector (and we have three components of it!) - lets work on getting the relative horizontal velocity between the two sites. We will fix MTJO and work out the velocity of HOKI relative to it. To do that we want to:\n",
    "1. Subtract the east and north velocities of MTFO from HOKI\n",
    "2. Do some trigonometry to work out the magnitude and orientation of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime(2001, 1, 1)\n",
    "endtime = datetime.datetime(2022, 1, 1)\n",
    "\n",
    "hoki = get_gnss_for_station(\n",
    "    \"HOKI\", starttime=starttime, endtime=endtime)\n",
    "mtjo = get_gnss_for_station(\n",
    "    \"MTJO\", starttime=starttime, endtime=endtime)\n",
    "\n",
    "hoki_df = pd.DataFrame.from_dict(hoki)\n",
    "mtjo_df = pd.DataFrame.from_dict(mtjo)\n",
    "\n",
    "hoki_df.set_index(\"time\", inplace=True)\n",
    "mtjo_df.set_index(\"time\", inplace=True)\n",
    "\n",
    "af_data = pd.merge(\n",
    "    left=hoki_df, right=mtjo_df, how='inner', \n",
    "    left_index=True, right_index=True,\n",
    "    suffixes=(\"_hoki\", \"_mtjo\"))\n",
    "\n",
    "# Subtract east and north velocities of MTJO from HOKI\n",
    "relative_east = af_data.east_hoki - af_data.east_mtjo\n",
    "relative_north = af_data.north_hoki - af_data.north_mtjo\n",
    "\n",
    "# Make a new dataframe that contains our relative horizontal velocities\n",
    "hoki_mtjo_relative = pd.DataFrame({\"east\": relative_east, \"north\": relative_north})\n",
    "\n",
    "# Because we just care about relative velocities we can set the data to zero at the beginning of observations\n",
    "hoki_mtjo_relative.east -= hoki_mtjo_relative.east[0]\n",
    "hoki_mtjo_relative.north -= hoki_mtjo_relative.north[0]\n",
    "\n",
    "ax = hoki_mtjo_relative.plot()\n",
    "ax.set_ylabel(\"Displacement (mm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Chocolate Fish for whoever tells me what that eastward step is...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the relative displacements we can work out the magnitude and direction of the relative displacement vector, then divide that magnitude by the sample interval to get the relative velocity magnitude.\n",
    "\n",
    "To work out the magnitude and bearing of the relative displacement we need to use simple trig. We want to apply that maths to the whole of our dataframe. Pandas has a handy way of doing this using the [`.apply()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html#pandas.DataFrame.apply) method. \n",
    "\n",
    "We are going to define two simple functions to get the magnitude and bearing of the displacement vector, then use the `.apply()` method on our `DataFrame` to  get this done efficiently. Pandas does fun things to accelerate the `.apply()` method that make it faster than just looping over our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import atan2, degrees\n",
    "\n",
    "def displacement_magnitude(row):\n",
    "    \"\"\" Calculate the magnitude of relative displacement. \"\"\"\n",
    "    return (row.east ** 2 + row.north ** 2) ** 0.5\n",
    "\n",
    "def displacement_bearing(row):\n",
    "    \"\"\" Calculate the orientation of relative displacement. \"\"\"\n",
    "    return degrees(atan2(row.east, row.north)) % 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoki_mtjo_relative[\"magnitude\"] = hoki_mtjo_relative.apply(displacement_magnitude, axis=1)\n",
    "\n",
    "hoki_mtjo_relative[\"bearing\"] = hoki_mtjo_relative.apply(displacement_bearing, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = hoki_mtjo_relative.magnitude.plot(ax=ax)\n",
    "ax.set_ylabel('Displacement (mm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fairly nice way to look at angular data is using a [polar](https://matplotlib.org/stable/gallery/pie_and_polar_charts/polar_demo.html) plot. We can look at the bearing of the relative displacement over time and have a check to see that it does what we expect - who can remember the sense of motion across the Alpine Fault, and the general orientation of the Alpine Fault?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "ax.plot(np.radians(hoki_mtjo_relative.bearing), hoki_mtjo_relative.index)\n",
    "ax.set_theta_zero_location('N')\n",
    "ax.set_theta_direction(-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply all sorts of function through DataFrames using this approach! First though we are going to calculate the relative velocity by simply dividing our relative displacement magnitude by the time interval - note that our data are not regularly sampled, so we can't just divide by 1 day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time elapsed between samples and convert from seconds to days\n",
    "delta_time = (hoki_mtjo_relative.index[1:] - hoki_mtjo_relative.index[0:-1]).total_seconds() / (365 * 86400)\n",
    "\n",
    "delta_displacement = hoki_mtjo_relative.magnitude[1:].to_numpy() - hoki_mtjo_relative.magnitude[0:-1].to_numpy()\n",
    "\n",
    "hoki_mtjo_relative[\"velocity\"] = pd.Series(\n",
    "    delta_displacement / delta_time, \n",
    "    index=hoki_mtjo_relative.index[1:])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = hoki_mtjo_relative.velocity.plot(ax=ax)\n",
    "ax.set_ylabel(\"Velocity (mm/day)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is pretty noisy again, lets look at a moving average of that velocity to smooth out the noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size_days = 7\n",
    "\n",
    "roll_median = hoki_mtjo_relative.velocity.rolling(\n",
    "    f\"{window_size_days}d\", min_periods=6).median()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = roll_median[0::window_size_days].plot(ax=ax)\n",
    "ax.set_ylabel(\"Velocity (mm/day)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a little less noisy and we can clearly see the step in velocity after KaikÅura. There also looks like ther might be a seasonal trend to me... Try looking at different moving window sizes to see what you see.\n",
    "\n",
    "\n",
    "## Exercise:\n",
    "\n",
    "Now you have a go - select two sites in NZ and compute the relative velocity using the methods shown above. See if you see anything interesting! Hint: you might want to try sites on either side of major faults, or one site close to the east coast of North Island and one in northland to see if you see any SSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin\n",
    "\n",
    "Nice. If you want another example of applying moving window functions check out Notebook 5. You can also find some Fourier analysis of periodic signals, like the seasonal signals we see in GNSS data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
