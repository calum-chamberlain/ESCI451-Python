{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python for Earth Scientists\n",
    "\n",
    "These notebooks have been developed by Calum Chamberlain, Finnigan Illsley-Kemp and John Townend at [Victoria University of Wellington-Te Herenga Waka](https://www.wgtn.ac.nz) for use by Earth Science graduate students. \n",
    "\n",
    "The notebooks cover material that we think will be of particular benefit to those students with little or no previous experience of computer-based data analysis. We presume very little background in command-line or code-based computing, and have compiled this material with an emphasis on general tasks that a grad student might encounter on a daily basis. \n",
    "\n",
    "In 2022, this material will be delivered at the start of Trimester 1 in conjunction with [ESCI451 Active Earth](https://www.wgtn.ac.nz/courses/esci/451/2022/offering?crn=32176). Space and pandemic alert levels permitting, interested students not enrolled in ESCI451 are encouraged to come along too but please contact Calum, Finn, or John first.\n",
    "\n",
    "| Notebook | Contents | Data |\n",
    "| --- | --- | --- |\n",
    "| [1A](ESCI451_Module_1A.ipynb) | Introduction to programming, Python, and Jupyter notebooks | - |\n",
    "| [1B](ESCI451_Module_1B.ipynb) | Basic data types and variables, getting data, and plotting with Matplotlib | Geodetic positions |\n",
    "| [2A](ESCI451_Module_2A.ipynb) | More complex plotting, introduction to Numpy | Geodetic positions; DFDP-2B temperatures |\n",
    "| **[2B](ESCI451_Module_2B.ipynb)** | **Using Pandas to load, peruse and plot data** | **Earthquake catalogue**  |\n",
    "| [3A](ESCI451_Module_3A.ipynb) | Working with Pandas dataframes | Geochemical data set; GNSS data |\n",
    "| [3B](ESCI451_Module_3B.ipynb) | Simple time series analysis using Pandas | Historical temperature records |\n",
    "| [4A](ESCI451_Module_4A.ipynb) | Making maps with PyGMT | Earthquake catalogue |\n",
    "| [4B](ESCI451_Module_4B.ipynb) | Gridded data and vectors | Ashfall data and GNSS |\n",
    "\n",
    "The content may change in response to students' questions or current events. Each of the four modules has been designed to take about three hours, with a short break between each of the two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook\n",
    "\n",
    "1. An introduction to Pandas and dataframes\n",
    "   - Loading data into a dataframe\n",
    "   - Visualising the datafame\n",
    "   - Dataframe statistics\n",
    "   - Sorting and slicing a dataframe\n",
    "   - Editing dataframes\n",
    "\n",
    "# An introduction to Pandas and dataframes\n",
    " \n",
    "<img alt=\"Pandas logo\" align=\"right\" style=\"width:30%\" src=\"https://dev.pandas.io/static/img/pandas.svg\">\n",
    "\n",
    "\n",
    "So far we have looked at some fairly simple datasets.  NumPy is great for multi-dimensional arrays, but\n",
    "book-keeping can be tricky and somewhat counterintuitive.  Pandas is our friend here.  Pandas adds meta-data to our data, and allows\n",
    "us to interact with data using names and words, rather than indexes. This can mean that we can\n",
    "write much clearer code (yay).  It's also really good at working with data that you would have previously\n",
    "interacted with in spreadsheets.  Spreadsheets are the source of **many** errors, and keeping data and\n",
    "results in the same file is almost criminal! Your data are sacred and should **never be in the same\n",
    "file that you process them in!**.\n",
    "\n",
    "Pandas [github README](https://github.com/pandas-dev/pandas/blob/master/README.md) outlines why you should\n",
    "care about Pandas:\n",
    "\n",
    "> **pandas** is a Python package providing fast, flexible, and expressive data structures designed to \n",
    "make working with \"relational\" or \"labeled\" data both easy and intuitive. It aims to be the \n",
    "fundamental high-level building block for doing practical, **real world** data analysis in Python.\n",
    "Additionally, it has the broader goal of becoming the **most powerful and flexible open source \n",
    "data analysis / manipulation tool available in any language**. It is already well on its way towards \n",
    "this goal.\n",
    "\n",
    "When Pandas says **real world** think messy data. Measurements of properties of the Earth are *almost always*\n",
    "messy: data are missed when power fails or equipment breaks or when it is too wet to get into the field,\n",
    "almost all Earth science datasets are noisy, and almost all Earth science data are multi-dimensional and\n",
    "relational (e.g. multiple variables at one particular place and/or time).  Pandas is really good at coping\n",
    "with this mess, and **will make your life easier!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactive plots.\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into a dataframe\n",
    "\n",
    "To explore some of the functionality of Pandas, we need a dataset. One large and freely accesible\n",
    "geoscience dataset in New Zealand is the GeoNet eatrhquake catalogue. This contains hundreds of thousands\n",
    "of earthquakes, so should be fun to play around with.\n",
    "\n",
    "To start off with, we need to get the data.  We could manually query the \n",
    "[Quake Search](https://quakesearch.geonet.org.nz/) web-app, but that means we need to\n",
    "click lots of buttons, and isn't great for just exploring a dataset.  Lets do it\n",
    "programatically.  We will build a function, but let's look at the steps along the way.\n",
    "\n",
    "### Building a query\n",
    "\n",
    "The Quake Search page can be queried by generating a specific web request in the form:\n",
    "\n",
    "`\"https://quakesearch.geonet.org.nz/csv?bbox={min-longitude},{min-latitude},{max-longitude},{max-latitude}&minmag={min-magnitude}&maxmag={max-magnitude}&mindepth={min-depth}&maxdepth={max-depth}&startdate={start-time}&enddate={end-time}\"`\n",
    "\n",
    "We can build that as a string really easily using variables in place of the curly-brackets things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_string = (\n",
    "    \"https://quakesearch.geonet.org.nz/csv?bbox=\"\n",
    "    \"{min_longitude},{min_latitude},{max_longitude},\"\n",
    "    \"{max_latitude}&minmag={min_magnitude}\"\n",
    "    \"&maxmag={max_magnitude}&mindepth={min_depth}\"\n",
    "    \"&maxdepth={max_depth}&startdate={start_time}\"\n",
    "    \"&enddate={end_time}\")\n",
    "\n",
    "min_latitude = -49.0\n",
    "max_latitude = -40.0\n",
    "min_longitude = 164.0\n",
    "max_longitude = 182.0\n",
    "min_magnitude = 0.0\n",
    "max_magnitude = 9.0\n",
    "min_depth = 0.0  # in km\n",
    "max_depth = 500.0\n",
    "start_time = \"2019-1-1T00:00:00\"\n",
    "end_time = \"2020-1-1T00:00:00\"\n",
    "\n",
    "query_string = format_string.format(\n",
    "    min_latitude=min_latitude,\n",
    "    max_latitude=max_latitude,\n",
    "    min_longitude=min_longitude,\n",
    "    max_longitude=max_longitude,\n",
    "    min_magnitude=min_magnitude,\n",
    "    max_magnitude=max_magnitude,\n",
    "    min_depth=min_depth,\n",
    "    max_depth=max_depth,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time)\n",
    "\n",
    "print(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what we did? We specified the format of the query string, then specified the particular search criteria we were interested in, and then put those two elements together to construct the query string. Because we have used variables in place of parts of the string, we can change our query really easily.  \n",
    "\n",
    "If you click that link we just constructed you should download a file called *earthquakes.csv*. What we really want though is to download that file and look at it in Python straight away.  To do that\n",
    "we can use the `requests` package to make a web-request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(query_string)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All being well, that should have output `<Response [200]>`. The value of 200 is simply a return code saying that things went as planned.\n",
    "\n",
    "The `Response` object contains the content that we requested from the web as a `.contents` attribute.  Lets have a look at the first 1000 elements of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the contents of the `earthquakes.csv` file and we can write it to a file in the data directory.  The\n",
    "contents that we have downloaded are in binary (which `print` converted a string before\n",
    "displaying it), so we have to open the file we want to write to using the `wb` argument, which means\n",
    "\"open the file in **b**inary mode with **w**rite permission\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/earthquakes.csv\", \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could read those data in using some convoluted looping and NumPy arrays, or we could\n",
    "just get Pandas to read it using the \n",
    "[pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n",
    "function.  This will quickly parse that large csv file into a Pandas \n",
    "[dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # It is a normal convention to rename pandas as pd for short\n",
    "\n",
    "earthquakes = pd.read_csv(\"data/earthquakes.csv\")\n",
    "\n",
    "print(earthquakes[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes are really handy ways of handling \"spreadshseet\"-type data, because they provide a convenient way of labelling the columns. Here we have printed out the first five rows (starting from zero, remember) of the `earthquakes` dataframe we have create. This shows the catalogue information for five earthquakes, arranged in columns labelled `publicid`, `eventtype`, `origintime`, etc. You can see a full list of the column names with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earthquakes = pd.read_csv(\"data/earthquakes.csv\")\n",
    "\n",
    "earthquakes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the contents of those columns pretty easily too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earthquakes = pd.read_csv(\"data/earthquakes.csv\")\n",
    "\n",
    "print(earthquakes[\"origintime\"][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how in this case we've specified both a column (`origintime`) and a number of rows (the first ten).\n",
    "\n",
    "Each column is a Pandas [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series) which is similar to a numpy array\n",
    "and has a lot of the same functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will note that the time columns (`origintime` and `modificationtime`) have not been\n",
    "read in (\"parsed\") in the most helpful way: we can see what the strings represent but can't yet treat them as dates or times directly. To get around this, we can tell pandas to read those columns in as `datetime` objects\n",
    "using the `parse_dates` argument.  While we're at it, we can also get rid of the warning about values in column\n",
    "0 having multiple dtypes (short for data types) by setting the `dtype` argument for the `publicid` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "\n",
    "print(earthquakes[\"origintime\"][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `dtype` of the `origintime` column is reported as `datetime64`, which is a 64-bit precision\n",
    "`datetime` number. We'll leave more detailed discussion of dates and times until the next module and for the time being we'll explore the dataframe itself in a bit more detail. \n",
    "\n",
    "Before we do that, however, let's quickly address one other minor formatting issue. You might notice that some of the column names have a leading space in them.  GeoNet doesn't format it's tables particularly nicely, and those leading spaces are annoying. Let's rename the columns to remove the spaces - first we can make a dictionary that maps the original name to the new name, then use the `.rename` method on the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "\n",
    "column_mapper = dict()\n",
    "for column in earthquakes.columns:\n",
    "    # Use the strip method to remove spaces\n",
    "    column_mapper[column] = column.strip()\n",
    "    \n",
    "earthquakes = earthquakes.rename(columns=column_mapper)\n",
    "print(earthquakes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation\n",
    "\n",
    "Now we have a nicely named dataframe, lets have a look at some of the data.\n",
    "First lets look at magnitude against time. We could use matplotlib directly, but pandas\n",
    "has some handy plotting shortcuts built in - I have put all the parts from above together here as you would in your own script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "column_mapper = dict()\n",
    "for column in earthquakes.columns:\n",
    "    # Use the strip method to remove spaces\n",
    "    column_mapper[column] = column.strip()\n",
    "    \n",
    "earthquakes = earthquakes.rename(columns=column_mapper)\n",
    "\n",
    "# Plot the data!\n",
    "\n",
    "earthquakes.plot(x=\"origintime\", y=\"magnitude\", kind=\"scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specified the `x` argument as the column name we wanted to plot on the x-axis, and\n",
    "`y` as the other column name.  Pandas has a few different plotting options that can\n",
    "be specified by the `kind` argument, you can find out more about them \n",
    "[here](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html).\n",
    "\n",
    "You can clearly see the large magnitude Kaikoura earthquake standing out from everything else.\n",
    "\n",
    "Another helpful plot might be a histogram. The syntax for that is pretty straightforward too and here are two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "column_mapper = dict()\n",
    "for column in earthquakes.columns:\n",
    "    # Use the strip method to remove spaces\n",
    "    column_mapper[column] = column.strip()\n",
    "    \n",
    "earthquakes = earthquakes.rename(columns=column_mapper)\n",
    "\n",
    "# Plot the data!\n",
    "\n",
    "earthquakes.hist(column='depth', bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "column_mapper = dict()\n",
    "for column in earthquakes.columns:\n",
    "    # Use the strip method to remove spaces\n",
    "    column_mapper[column] = column.strip()\n",
    "    \n",
    "earthquakes = earthquakes.rename(columns=column_mapper)\n",
    "\n",
    "# Plot the data!\n",
    "\n",
    "earthquakes.hist(column=['depth', 'magnitude'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Pick a specific region based on latitude and longitude ([this website](http://bboxfinder.com/) is\n",
    "really helpful for finding bounding boxes) and get a dataframe spanning a longer period of\n",
    "time.  Plot the magnitude vs. time graph for that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here.  Call your dataframe something different to `earthquakes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe statistics\n",
    "\n",
    "We can also obtain some basic stats from our dataframe, like the median magnitude..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "column_mapper = dict()\n",
    "for column in earthquakes.columns:\n",
    "    # Use the strip method to remove spaces\n",
    "    column_mapper[column] = column.strip()\n",
    "    \n",
    "earthquakes = earthquakes.rename(columns=column_mapper)\n",
    "\n",
    "# Calculate the median!\n",
    "print(earthquakes[\"magnitude\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or the maximum depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(earthquakes['depth'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of other useful ways you can extract descriptive statistics from your dataframe, which are documented [here](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#computations-descriptive-stats).\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "What is the mean, maximum and minimum magnitude in our dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the dateframe\n",
    "\n",
    "Something we often need to do is to sort a dataset based on the value of one parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "column_mapper = dict()\n",
    "for column in earthquakes.columns:\n",
    "    # Use the strip method to remove spaces\n",
    "    column_mapper[column] = column.strip()\n",
    "    \n",
    "earthquakes = earthquakes.rename(columns=column_mapper)\n",
    "\n",
    "earthquakes.sort_values(by=[\"latitude\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see how we have sorted the dataframe, but the index remains as it was? We can fix that so that the index is reset by passing the `ignore_index` argument to `.sort_values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes.sort_values(by=[\"latitude\"], ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Sort the dataframe by depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing dataframes\n",
    "\n",
    "We can also select subsets of our dataframe; this is commonly referred to as \"slicing\".  Say you had downloaded the whole catalogue\n",
    "and realised that you only wanted events shallower than 20 km depth. The `.loc` command is used to slice the dataframe to only those rows meeting the specific criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "column_mapper = dict()\n",
    "for column in earthquakes.columns:\n",
    "    # Use the strip method to remove spaces\n",
    "    column_mapper[column] = column.strip()\n",
    "    \n",
    "earthquakes = earthquakes.rename(columns=column_mapper)\n",
    "\n",
    "earthquakes.loc[earthquakes[\"depth\"] <= 20.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain multiple conditions together using the \"&\" operator. Here's what we can do if we only want the earthquakes shallower than 20 km and \n",
    "larger than magnitude 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "column_mapper = dict()\n",
    "for column in earthquakes.columns:\n",
    "    # Use the strip method to remove spaces\n",
    "    column_mapper[column] = column.strip()\n",
    "    \n",
    "earthquakes = earthquakes.rename(columns=column_mapper)\n",
    "\n",
    "earthquakes.loc[(earthquakes[\"depth\"] <= 20.0) &\n",
    "                (earthquakes[\"magnitude\"] > 4.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Select earthquakes deeper than 80 km depth between -42 degrees latitude and -44 degrees latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing dataframes\n",
    "\n",
    "Sometimes you need to add or remove rows of columns from your dataframe, or you might want to change how the dataframe is indexed. This kind of dataframe editing is relatively straightforward in Pandas, although there are often multiple ways to accomplish the same goal. In this section we will try to demonstrate some of these methods.\n",
    "\n",
    "### Removing columns\n",
    "\n",
    "Say we didn't care about the \"modificationtime\" column - we can use the [`.drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method to get rid of columns or rows - in this case we can pass `columns=\"modificationtime\"` to remove that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "column_mapper = dict()\n",
    "for column in earthquakes.columns:\n",
    "    # Use the strip method to remove spaces\n",
    "    column_mapper[column] = column.strip()\n",
    "    \n",
    "earthquakes = earthquakes.rename(columns=column_mapper)\n",
    "\n",
    "earthquakes = earthquakes.drop(columns=\"modificationtime\")\n",
    "print(earthquakes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know that we only want a few columns then a simpler way than dropping lots of columns might be to just read in the columns we care about, and we can do that in `.read_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    usecols=[\"origintime\", \"magnitude\", \"latitude\", \"longitude\", \"depth\", \"publicid\"],\n",
    "    parse_dates=[\"origintime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "\n",
    "print(earthquakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often find that we want to add more information to our dataframes, for example we might want to add a column that is the distance away from some place we care about, say VUW... We will talk more about applying functions to dataframes in a couple of notebooks time, but for now we will import a function for calculating the distance and loop through our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.geodetics import globe_distance\n",
    "import pandas as pd\n",
    "\n",
    "earthquakes = pd.read_csv(\n",
    "    \"data/earthquakes.csv\",\n",
    "    usecols=[\"origintime\", \"magnitude\", \"latitude\", \"longitude\", \"depth\", \"publicid\"],\n",
    "    parse_dates=[\"origintime\"],\n",
    "    dtype={\"publicid\": str})\n",
    "\n",
    "vuw_lat, vuw_lon = -41.2901, 174.768\n",
    "\n",
    "distances = []\n",
    "for row in earthquakes.itertuples():\n",
    "    distance = globe_distance(vuw_lat, vuw_lon, row.latitude, row.longitude)\n",
    "    distances.append(distance)\n",
    "    \n",
    "print(distances[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add that column into our dataframe we can simply assign a new column as if the dataframe was a dictionary using syntax like:\n",
    "\n",
    "`dataframe[\"new_column\"] = distances`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes[\"distance\"] = distances\n",
    "\n",
    "earthquakes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all of the above - what is the largest magnitude earthquake that happened within 100 km of VUW in our data period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here - hint, use slicing to get events within 100km and find the max of the magnitude column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
